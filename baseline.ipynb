{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'torch.tensortype' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerated_text_detector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroberta_classifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/NLP/.venv/lib/python3.9/site-packages/generated_text_detector/utils/model/roberta_classifier.py:10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerated_text_detector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbce_smoothed_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLossSmoothed\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRobertaClassifier\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule, PyTorchModelHubMixin):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Roberta based text classifier.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    :param config: Configuration dictionary containing model parameters\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        should contain following keys: `pretrain_checkpoint`, `classifier_dropout`, `num_labels`, `label_smoothing`\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    :type config: dict\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/github/NLP/.venv/lib/python3.9/site-packages/generated_text_detector/utils/model/roberta_classifier.py:29\u001b[0m, in \u001b[0;36mRobertaClassifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func \u001b[38;5;241m=\u001b[39m BCEWithLogitsLossSmoothed(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_smoothing\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m---> 29\u001b[0m     input_ids: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m     attention_mask: torch\u001b[38;5;241m.\u001b[39mFloatTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m     token_type_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m     position_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m     head_mask: torch\u001b[38;5;241m.\u001b[39mFloatTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m     inputs_embeds: torch\u001b[38;5;241m.\u001b[39mFloatTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     labels: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m     output_hidden_states: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m     return_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m     cls_output: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of the classifier.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :param input_ids: Input token IDs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    :rtype: Union[SequenceClassifierOutput, Tuple[torch.Tensor, torch.Tensor]]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Forward pass through Roberta model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'torch.tensortype' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from generated_text_detector.utils.model.roberta_classifier import RobertaClassifier\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model = RobertaClassifier.from_pretrained(\"SuperAnnotate/roberta-large-llm-content-detector\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SuperAnnotate/roberta-large-llm-content-detector\")\n",
    "\n",
    "text_example = \"It's not uncommon for people to develop allergies or intolerances to certain foods as they get older. It's possible that you have always had a sensitivity to lactose (the sugar found in milk and other dairy products), but it only recently became a problem for you. This can happen because our bodies can change over time and become more or less able to tolerate certain things. It's also possible that you have developed an allergy or intolerance to something else that is causing your symptoms, such as a food additive or preservative. In any case, it's important to talk to a doctor if you are experiencing new allergy or intolerance symptoms, so they can help determine the cause and recommend treatment.\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "    text_example,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
