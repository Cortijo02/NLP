{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ijaKWX_ecbac",
    "outputId": "8d5bcab8-8fae-4119-bc00-535d4ef89622"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support, brier_score_loss\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuración de dispositivo (GPU o CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "q_zVgjhtcbae"
   },
   "outputs": [],
   "source": [
    "# @title Customize your key variables here\n",
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512 # @param {type:\"integer\"}\n",
    "TRAIN_BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
    "VALID_BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
    "EPOCHS = 6 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 1e-5 # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGICgd8wcbae",
    "outputId": "21ca6e93-8db1-4ba7-cf32-983174f34156"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines_files = glob('../data/machine/*.jsonl')\n",
    "len(machines_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1087 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id\n",
       "0     news-2021-01-01-2021-12-31-bideninauguration/a...\n",
       "1     news-2021-01-01-2021-12-31-bideninauguration/a...\n",
       "2     news-2021-01-01-2021-12-31-bideninauguration/a...\n",
       "3     news-2021-01-01-2021-12-31-bideninauguration/a...\n",
       "4     news-2021-01-01-2021-12-31-bideninauguration/a...\n",
       "...                                                 ...\n",
       "1082  news-2021-01-01-2021-12-31-wyominggabbypetito/...\n",
       "1083  news-2021-01-01-2021-12-31-wyominggabbypetito/...\n",
       "1084  news-2021-01-01-2021-12-31-wyominggabbypetito/...\n",
       "1085  news-2021-01-01-2021-12-31-wyominggabbypetito/...\n",
       "1086  news-2021-01-01-2021-12-31-wyominggabbypetito/...\n",
       "\n",
       "[1087 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ids = pd.read_json('../data/human.jsonl', lines=True)\n",
    "df_ids = df_ids[['id']]\n",
    "df_ids[\"id\"] = df_ids[\"id\"].str.split('/').str[1:].str.join('/')\n",
    "df_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rsuB0RLcbaf",
    "outputId": "0bd62678-67bf-4312-afc8-c55b042f648f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 1), (218, 1), (109, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids_df, temp_df = train_test_split(df_ids, test_size=0.3, random_state=42)\n",
    "val_ids_df, test_ids_df = train_test_split(temp_df, test_size=1/3, random_state=42)\n",
    "\n",
    "train_ids_df.shape, val_ids_df.shape, test_ids_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "Ouq8Ii2lcbae",
    "outputId": "b671c7f6-9177-437f-846f-9e51effa9b8c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_human = pd.read_json('../data/human.jsonl', lines=True)\n",
    "\n",
    "df_human_temp = df_human.reset_index(drop=True)\n",
    "df_human_temp['label'] = 0\n",
    "\n",
    "df_machine = pd.read_json(machines_files[0], lines=True)\n",
    "for file in machines_files[1:]:\n",
    "    df_current = pd.read_json(file, lines=True)\n",
    "    df_machine = pd.concat([df_machine, df_current])\n",
    "\n",
    "df_machine_temp = df_machine.reset_index(drop=True)\n",
    "df_machine_temp['label'] = 1\n",
    "\n",
    "\n",
    "df_temp = pd.concat([df_human_temp, df_machine_temp], axis=0, ignore_index=True)\n",
    "\n",
    "# Calcular la longitud del texto en la columna 'text'\n",
    "df_temp[\"text_length\"] = df_temp[\"text\"].apply(len)\n",
    "\n",
    "# Crear una figura con 2 subgráficos (uno al lado del otro)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Gráfico de la longitud del texto (sin `palette`)\n",
    "sns.boxplot(data=df_temp, x=\"label\", hue=\"label\",y=\"text_length\", ax=ax[0], legend=False)\n",
    "ax[0].set_title(\"Distribución de Longitud de Texto por Clase\")\n",
    "ax[0].set_xlabel(\"Etiqueta (0: Humano, 1: IA)\")\n",
    "ax[0].set_ylabel(\"Longitud del Texto (Caracteres)\")\n",
    "\n",
    "# 2. Gráfico de la cantidad de texto generado por IA y Humanos\n",
    "sns.countplot(data=df_temp, x=\"label\", hue=\"label\", palette=\"viridis\", ax=ax[1], legend=False)\n",
    "ax[1].set_title(\"Cantidad de Textos Generados por Humanos vs IA\")\n",
    "ax[1].set_xlabel(\"Etiqueta (0: Humano, 1: IA)\")\n",
    "ax[1].set_ylabel(\"Cantidad de Textos\")\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_1</th>\n",
       "      <th>target_human</th>\n",
       "      <th>text_2</th>\n",
       "      <th>target_machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>Inaugural Address: President Joseph R. Biden J...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>What should be the focus of the speech? The In...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>Biden's Inaugural Address Highlights Triumph o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>Biden's Inaugural Address: A Clarion Call for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>President Biden Emphasizes Unity, Democracy, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gabby Petito: Long Island Surf Shop Owner Reme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gabby Petito: Surf Shop Owner in Hometown Reme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gabby Petito Remembered as a 'Kind-Hearted Sou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14129</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gabby Petito Remembered as a 'Super Kind-Heart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Very Kind and Sweet Woman in Long Island Sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id  \\\n",
       "0      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "1      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "2      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "3      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "4      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "...                                                  ...   \n",
       "14126  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14127  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14128  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14129  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14130  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "\n",
       "                                                  text_1  target_human  \\\n",
       "0      Inaugural Address by President Joseph R. Biden...             1   \n",
       "1      Inaugural Address by President Joseph R. Biden...             1   \n",
       "2      Inaugural Address by President Joseph R. Biden...             1   \n",
       "3      Inaugural Address by President Joseph R. Biden...             1   \n",
       "4      Inaugural Address by President Joseph R. Biden...             1   \n",
       "...                                                  ...           ...   \n",
       "14126  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "14127  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "14128  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "14129  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "14130  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "\n",
       "                                                  text_2  target_machine  \n",
       "0      Inaugural Address: President Joseph R. Biden J...               0  \n",
       "1      What should be the focus of the speech? The In...               0  \n",
       "2      Biden's Inaugural Address Highlights Triumph o...               0  \n",
       "3      Biden's Inaugural Address: A Clarion Call for ...               0  \n",
       "4      President Biden Emphasizes Unity, Democracy, a...               0  \n",
       "...                                                  ...             ...  \n",
       "14126  Gabby Petito: Long Island Surf Shop Owner Reme...               0  \n",
       "14127  Gabby Petito: Surf Shop Owner in Hometown Reme...               0  \n",
       "14128  Gabby Petito Remembered as a 'Kind-Hearted Sou...               0  \n",
       "14129  Gabby Petito Remembered as a 'Super Kind-Heart...               0  \n",
       "14130  A Very Kind and Sweet Woman in Long Island Sho...               0  \n",
       "\n",
       "[14131 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human[\"id\"] = df_human[\"id\"].str.split('/').str[1:].str.join('/')\n",
    "df_machine[\"id\"] = df_machine[\"id\"].str.split('/').str[1:].str.join('/')\n",
    "\n",
    "df_combined = pd.merge(df_human, df_machine, on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "df_combined['target_human'] = 1\n",
    "df_combined['target_machine'] = 0\n",
    "df_combined = df_combined[['id', 'text_1', 'target_human', 'text_2', 'target_machine']]\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "CB8z35Ggcbae",
    "outputId": "d0dccaae-b1db-4720-c8ba-f2f24ab40231"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_1</th>\n",
       "      <th>target_human</th>\n",
       "      <th>text_2</th>\n",
       "      <th>target_machine</th>\n",
       "      <th>target_tuple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address: President Joseph R. Biden J...</td>\n",
       "      <td>0</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>What should be the focus of the speech? The In...</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>Biden's Inaugural Address Highlights Triumph o...</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Biden's Inaugural Address: A Clarion Call for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>1</td>\n",
       "      <td>President Biden Emphasizes Unity, Democracy, a...</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito: Long Island Surf Shop Owner Reme...</td>\n",
       "      <td>0</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gabby Petito: Surf Shop Owner in Hometown Reme...</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gabby Petito Remembered as a 'Kind-Hearted Sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14129</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito Remembered as a 'Super Kind-Heart...</td>\n",
       "      <td>0</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>A Very Kind and Sweet Woman in Long Island Sho...</td>\n",
       "      <td>0</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14131 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id  \\\n",
       "0      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "1      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "2      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "3      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "4      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "...                                                  ...   \n",
       "14126  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14127  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14128  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14129  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14130  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "\n",
       "                                                  text_1  target_human  \\\n",
       "0      Inaugural Address: President Joseph R. Biden J...             0   \n",
       "1      Inaugural Address by President Joseph R. Biden...             1   \n",
       "2      Inaugural Address by President Joseph R. Biden...             1   \n",
       "3      Biden's Inaugural Address: A Clarion Call for ...             0   \n",
       "4      Inaugural Address by President Joseph R. Biden...             1   \n",
       "...                                                  ...           ...   \n",
       "14126  Gabby Petito: Long Island Surf Shop Owner Reme...             0   \n",
       "14127  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "14128  Gabby Petito case: Surf shop owner in her home...             1   \n",
       "14129  Gabby Petito Remembered as a 'Super Kind-Heart...             0   \n",
       "14130  A Very Kind and Sweet Woman in Long Island Sho...             0   \n",
       "\n",
       "                                                  text_2  target_machine  \\\n",
       "0      Inaugural Address by President Joseph R. Biden...               1   \n",
       "1      What should be the focus of the speech? The In...               0   \n",
       "2      Biden's Inaugural Address Highlights Triumph o...               0   \n",
       "3      Inaugural Address by President Joseph R. Biden...               1   \n",
       "4      President Biden Emphasizes Unity, Democracy, a...               0   \n",
       "...                                                  ...             ...   \n",
       "14126  Gabby Petito case: Surf shop owner in her home...               1   \n",
       "14127  Gabby Petito: Surf Shop Owner in Hometown Reme...               0   \n",
       "14128  Gabby Petito Remembered as a 'Kind-Hearted Sou...               0   \n",
       "14129  Gabby Petito case: Surf shop owner in her home...               1   \n",
       "14130  Gabby Petito case: Surf shop owner in her home...               1   \n",
       "\n",
       "      target_tuple  \n",
       "0           (0, 1)  \n",
       "1           (1, 0)  \n",
       "2           (1, 0)  \n",
       "3           (0, 1)  \n",
       "4           (1, 0)  \n",
       "...            ...  \n",
       "14126       (0, 1)  \n",
       "14127       (1, 0)  \n",
       "14128       (1, 0)  \n",
       "14129       (0, 1)  \n",
       "14130       (0, 1)  \n",
       "\n",
       "[14131 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_indices = df_combined.sample(frac=0.5, random_state=42).index\n",
    "df_combined.loc[random_indices, ['text_1', 'text_2']] = df_combined.loc[random_indices, ['text_2', 'text_1']].values\n",
    "df_combined.loc[random_indices, ['target_human']] = 0\n",
    "df_combined.loc[random_indices, ['target_machine']] = 1\n",
    "df_combined['target_tuple'] = list(zip(df_combined['target_human'], df_combined['target_machine']))\n",
    "df_combined = df_combined[['id', 'text_1', 'target_human', 'text_2', 'target_machine', 'target_tuple']]\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9880, 3), (2834, 3), (1417, 3))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_combined[df_combined['id'].isin(train_ids_df['id'])].drop(columns=['id', 'target_human', 'target_machine'])\n",
    "X_val = df_combined[df_combined['id'].isin(val_ids_df['id'])].drop(columns=['id', 'target_human', 'target_machine'])\n",
    "X_test = df_combined[df_combined['id'].isin(test_ids_df['id'])].drop(columns=['id', 'target_human', 'target_machine'])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>target_tuple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8504</th>\n",
       "      <td>Judge Denies Requests to Use Evidence Aiming t...</td>\n",
       "      <td>Kenosha County judge denies prosecutor's reque...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10396</th>\n",
       "      <td>You're Not Getting a 4th Stimulus Check. Here'...</td>\n",
       "      <td>` Fourth Round of Federal Stimulus Checks Unli...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>Evidence in Chauvin case contradicted first po...</td>\n",
       "      <td>Video Evidence Critical in Revealing Truth of ...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>President Biden Unveils Bold Plan to Accelerat...</td>\n",
       "      <td>FACT SHEET: President Biden Announces Increase...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>Kathy Hochul Sworn In as New York's First Fema...</td>\n",
       "      <td>Kathy Hochul becomes governor of New York as C...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text_1  \\\n",
       "8504   Judge Denies Requests to Use Evidence Aiming t...   \n",
       "10396  You're Not Getting a 4th Stimulus Check. Here'...   \n",
       "5345   Evidence in Chauvin case contradicted first po...   \n",
       "3632   President Biden Unveils Bold Plan to Accelerat...   \n",
       "1595   Kathy Hochul Sworn In as New York's First Fema...   \n",
       "\n",
       "                                                  text_2 target_tuple  \n",
       "8504   Kenosha County judge denies prosecutor's reque...       (0, 1)  \n",
       "10396  ` Fourth Round of Federal Stimulus Checks Unli...       (1, 0)  \n",
       "5345   Video Evidence Critical in Revealing Truth of ...       (1, 0)  \n",
       "3632   FACT SHEET: President Biden Announces Increase...       (0, 1)  \n",
       "1595   Kathy Hochul becomes governor of New York as C...       (0, 1)  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9880,), (2834,), (1417,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = X_train['target_tuple']\n",
    "y_val = X_val['target_tuple']\n",
    "y_test = X_test['target_tuple']\n",
    "\n",
    "X_train = X_train.drop(columns=['target_tuple'])\n",
    "X_val = X_val.drop(columns=['target_tuple'])\n",
    "X_test = X_test.drop(columns=['target_tuple'])\n",
    "\n",
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaugural Address: President Joseph R. Biden J...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>What should be the focus of the speech? The In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>Biden's Inaugural Address Highlights Triumph o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden's Inaugural Address: A Clarion Call for ...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>President Biden Emphasizes Unity, Democracy, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14100</th>\n",
       "      <td>Univ. of Wisconsin Oshkosh student helping Gab...</td>\n",
       "      <td>University of Wisconsin Oshkosh Student Claims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14101</th>\n",
       "      <td>TESLA STOCK SOARS ON Q3 EARNINGS REPORT\\n\\nWYO...</td>\n",
       "      <td>Univ. of Wisconsin Oshkosh student helping Gab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14102</th>\n",
       "      <td>WYOMING (WBAY) – A University of Wisconsin Osh...</td>\n",
       "      <td>Univ. of Wisconsin Oshkosh student helping Gab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14103</th>\n",
       "      <td>Univ. of Wisconsin Oshkosh student helping Gab...</td>\n",
       "      <td>University of Wisconsin Oshkosh Student Claims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14104</th>\n",
       "      <td>Univ. of Wisconsin Oshkosh student helping Gab...</td>\n",
       "      <td>McKenna's Lost Friend: Debunking the Evidence ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9880 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text_1  \\\n",
       "0      Inaugural Address: President Joseph R. Biden J...   \n",
       "1      Inaugural Address by President Joseph R. Biden...   \n",
       "2      Inaugural Address by President Joseph R. Biden...   \n",
       "3      Biden's Inaugural Address: A Clarion Call for ...   \n",
       "4      Inaugural Address by President Joseph R. Biden...   \n",
       "...                                                  ...   \n",
       "14100  Univ. of Wisconsin Oshkosh student helping Gab...   \n",
       "14101  TESLA STOCK SOARS ON Q3 EARNINGS REPORT\\n\\nWYO...   \n",
       "14102  WYOMING (WBAY) – A University of Wisconsin Osh...   \n",
       "14103  Univ. of Wisconsin Oshkosh student helping Gab...   \n",
       "14104  Univ. of Wisconsin Oshkosh student helping Gab...   \n",
       "\n",
       "                                                  text_2  \n",
       "0      Inaugural Address by President Joseph R. Biden...  \n",
       "1      What should be the focus of the speech? The In...  \n",
       "2      Biden's Inaugural Address Highlights Triumph o...  \n",
       "3      Inaugural Address by President Joseph R. Biden...  \n",
       "4      President Biden Emphasizes Unity, Democracy, a...  \n",
       "...                                                  ...  \n",
       "14100  University of Wisconsin Oshkosh Student Claims...  \n",
       "14101  Univ. of Wisconsin Oshkosh student helping Gab...  \n",
       "14102  Univ. of Wisconsin Oshkosh student helping Gab...  \n",
       "14103  University of Wisconsin Oshkosh Student Claims...  \n",
       "14104  McKenna's Lost Friend: Debunking the Evidence ...  \n",
       "\n",
       "[9880 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_tuple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14100</th>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14101</th>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14102</th>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14103</th>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14104</th>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9880 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target_tuple\n",
       "0           (0, 1)\n",
       "1           (1, 0)\n",
       "2           (1, 0)\n",
       "3           (0, 1)\n",
       "4           (1, 0)\n",
       "...            ...\n",
       "14100       (1, 0)\n",
       "14101       (0, 1)\n",
       "14102       (0, 1)\n",
       "14103       (1, 0)\n",
       "14104       (1, 0)\n",
       "\n",
       "[9880 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "LjuGLfavcbaf"
   },
   "outputs": [],
   "source": [
    "class AiClassificationDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Use iloc to access the rows by index for data and labels\n",
    "        text_1 = self.data.iloc[index]['text_1']\n",
    "        text_2 = self.data.iloc[index]['text_2']\n",
    "        target = self.labels.iloc[index]  # assuming labels are in a compatible format\n",
    "        return {\n",
    "            'text_1': text_1,\n",
    "            'text_2': text_2,\n",
    "            'targets': target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3U3xQ8HPcbaf"
   },
   "outputs": [],
   "source": [
    "class AiClassificationCollator:\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataset\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, input_batch):\n",
    "        batch_dict = {colname: [x[colname] for x in input_batch] for colname in input_batch[0]}\n",
    "\n",
    "        # Process text_1\n",
    "        comment_text_1 = batch_dict['text_1']\n",
    "        comment_text_1 = [\" \".join(text.split()) for text in comment_text_1]\n",
    "\n",
    "        # Process text_2\n",
    "        comment_text_2 = batch_dict['text_2']\n",
    "        comment_text_2 = [\" \".join(text.split()) for text in comment_text_2]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            comment_text_1,\n",
    "            comment_text_2,\n",
    "            max_length=self.max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n",
    "            'targets': torch.tensor(batch_dict['targets'], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8V7aggMfcbaf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "qXmpQsNzcbaf"
   },
   "outputs": [],
   "source": [
    "training_set = AiClassificationDataset(X_train, y_train)\n",
    "validation_set = AiClassificationDataset(X_val, y_val)\n",
    "test_set = AiClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': AiClassificationCollator(training_set, tokenizer, MAX_LEN)\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': AiClassificationCollator(validation_set, tokenizer, MAX_LEN)\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': AiClassificationCollator(test_set, tokenizer, MAX_LEN)\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loader Batch:\n",
      "ids: torch.Size([32, 512])\n",
      "mask: torch.Size([32, 512])\n",
      "token_type_ids: torch.Size([32, 512])\n",
      "targets: torch.Size([32, 2])\n",
      "\n",
      "Validation Loader Batch:\n",
      "ids: torch.Size([32, 512])\n",
      "mask: torch.Size([32, 512])\n",
      "token_type_ids: torch.Size([32, 512])\n",
      "targets: torch.Size([32, 2])\n",
      "\n",
      "Test Loader Batch:\n",
      "ids: torch.Size([32, 512])\n",
      "mask: torch.Size([32, 512])\n",
      "token_type_ids: torch.Size([32, 512])\n",
      "targets: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in training_loader:\n",
    "    print(\"Training Loader Batch:\")\n",
    "    if isinstance(batch, dict):  \n",
    "        for key, value in batch.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"{key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"{key}: {type(value)}\")\n",
    "    elif isinstance(batch, (list, tuple)):  \n",
    "        for i, item in enumerate(batch):\n",
    "            if hasattr(item, 'shape'):\n",
    "                print(f\"Item {i}: {item.shape}\")\n",
    "            else:\n",
    "                print(f\"Item {i}: {type(item)}\")\n",
    "    break  \n",
    "\n",
    "for batch in validation_loader:\n",
    "    print(\"\\nValidation Loader Batch:\")\n",
    "    if isinstance(batch, dict):\n",
    "        for key, value in batch.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"{key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"{key}: {type(value)}\")\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        for i, item in enumerate(batch):\n",
    "            if hasattr(item, 'shape'):\n",
    "                print(f\"Item {i}: {item.shape}\")\n",
    "            else:\n",
    "                print(f\"Item {i}: {type(item)}\")\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    print(\"\\nTest Loader Batch:\")\n",
    "    if isinstance(batch, dict):\n",
    "        for key, value in batch.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"{key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"{key}: {type(value)}\")\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        for i, item in enumerate(batch):\n",
    "            if hasattr(item, 'shape'):\n",
    "                print(f\"Item {i}: {item.shape}\")\n",
    "            else:\n",
    "                print(f\"Item {i}: {type(item)}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "KCmZiciwcbaf"
   },
   "outputs": [],
   "source": [
    "class TransformerClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 256)\n",
    "        self.l4 = torch.nn.Linear(256, 2)\n",
    "\n",
    "        print(\"sads\")\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        last_hidden_state = self.l1(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ).last_hidden_state\n",
    "\n",
    "        sentence_embedding = torch.sum(last_hidden_state*mask.unsqueeze(-1), 1) / torch.sum(mask.unsqueeze(-1),1)\n",
    "        hidden_output = F.gelu(self.l3(self.l2(sentence_embedding)))\n",
    "        output = self.l4(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "EsVknC-0eJPY"
   },
   "outputs": [],
   "source": [
    "def training_step(input_ids, attention_mask, token_type_ids, y, model, optimizer):\n",
    "    logits = model(input_ids, attention_mask, token_type_ids)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y, reduction='mean')\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ypAtTnOtcbag"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sads\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClass()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "QvHAB-s6cbag"
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            input_ids = data['ids'].to(device)\n",
    "            attention_mask = data['mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            val_loss += torch.nn.functional.cross_entropy(logits, targets, reduction='sum').item()  # Accumulate validation loss\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == targets.argmax(dim=1)).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / total_predictions\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ClviN54hFd6",
    "outputId": "fa97052b-3582-43bc-a74e-e37e5d035441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Step 1/309\n",
      "  Running Loss: 0.6925\n",
      "Epoch 1/6, Step 201/309\n",
      "  Running Loss: 0.3380\n",
      "Epoch 1/6 - End of epoch\n",
      "  Training Loss: 0.2300\n",
      "  Validation Loss: 0.0292\n",
      "  Validation Accuracy: 0.9912\n",
      "Model saved to ./model_weights_emb/model_epoch_1_acc0.9912.pth\n",
      "Epoch 2/6, Step 1/309\n",
      "  Running Loss: 0.0067\n",
      "Epoch 2/6, Step 201/309\n",
      "  Running Loss: 0.0146\n",
      "Epoch 2/6 - End of epoch\n",
      "  Training Loss: 0.0126\n",
      "  Validation Loss: 0.0605\n",
      "  Validation Accuracy: 0.9824\n",
      "Epoch 3/6, Step 1/309\n",
      "  Running Loss: 0.0017\n",
      "Epoch 3/6, Step 201/309\n",
      "  Running Loss: 0.0033\n",
      "Epoch 3/6 - End of epoch\n",
      "  Training Loss: 0.0065\n",
      "  Validation Loss: 0.1548\n",
      "  Validation Accuracy: 0.9552\n",
      "Epoch 4/6, Step 1/309\n",
      "  Running Loss: 0.0084\n",
      "Epoch 4/6, Step 201/309\n",
      "  Running Loss: 0.0012\n",
      "Epoch 4/6 - End of epoch\n",
      "  Training Loss: 0.0010\n",
      "  Validation Loss: 0.1823\n",
      "  Validation Accuracy: 0.9647\n",
      "Epoch 5/6, Step 1/309\n",
      "  Running Loss: 0.0010\n",
      "Epoch 5/6, Step 201/309\n",
      "  Running Loss: 0.0079\n",
      "Epoch 5/6 - End of epoch\n",
      "  Training Loss: 0.0069\n",
      "  Validation Loss: 0.0139\n",
      "  Validation Accuracy: 0.9965\n",
      "Model saved to ./model_weights_emb/model_epoch_5_acc0.9965.pth\n",
      "Epoch 6/6, Step 1/309\n",
      "  Running Loss: 0.0011\n",
      "Epoch 6/6, Step 201/309\n",
      "  Running Loss: 0.0005\n",
      "Epoch 6/6 - End of epoch\n",
      "  Training Loss: 0.0004\n",
      "  Validation Loss: 0.0166\n",
      "  Validation Accuracy: 0.9965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo de log\n",
    "log_file_path = './training_log.txt'\n",
    "\n",
    "def log_message(message):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "def train(epoch, log_interval=200, save_model_path='./model_weights'):\n",
    "    global best_accuracy\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for step, data in enumerate(training_loader):\n",
    "        input_ids = data['ids'].to(device)\n",
    "        attention_mask = data['mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        loss = training_step(input_ids, attention_mask, token_type_ids, targets, model, optimizer)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Almacenar la pérdida cada cierto número de pasos\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = running_loss / (step + 1)\n",
    "            log_message(f\"Epoch {epoch + 1}/{EPOCHS}, Step {step + 1}/{len(training_loader)}\")\n",
    "            log_message(f\"  Running Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(training_loader)\n",
    "\n",
    "    avg_val_loss, val_accuracy = validate()\n",
    "\n",
    "    log_message(f\"Epoch {epoch + 1}/{EPOCHS} - End of epoch\")\n",
    "    log_message(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "    log_message(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    log_message(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        if not os.path.exists(save_model_path):\n",
    "            os.makedirs(save_model_path)\n",
    "\n",
    "        model_save_path = os.path.join(save_model_path, f\"model_epoch_{epoch + 1}_acc{best_accuracy:.4f}.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        log_message(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "# Al iniciar, limpia el archivo de log si ya existe\n",
    "if os.path.exists(log_file_path):\n",
    "    open(log_file_path, 'w').close()\n",
    "\n",
    "\"\"\"Descomentar para entrenar\"\"\"\n",
    "# for epoch in range(EPOCHS):\n",
    "#        train(epoch)\n",
    "\n",
    "file_path = f'..\\logs\\BERT_B_Embedding_training_logs.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps_32 = [1, 201, 309, 310 + 1, 310 + 201, \n",
    "              2 * 309 + 310, 2 * 309 + 310 + 1, 2 * 309 + 310 + 201, \n",
    "              3 * 309 + 310, 3 * 309 + 310 + 1, 3 * 309 + 310 + 201, \n",
    "              4 * 309 + 310, 4 * 309 + 310 + 1, 4 * 309 + 310 + 201,\n",
    "              5 * 309 + 310, 5 * 309 + 310 + 1, 5 * 309 + 310 + 201,\n",
    "              6 * 309]\n",
    "\n",
    "losses = [\n",
    "    0.6925, 0.3380, 0.2300, \n",
    "    0.0067, 0.0146, 0.0126,\n",
    "    0.0017, 0.0033, 0.0065,\n",
    "    0.0084, 0.0012, 0.0010,\n",
    "    0.0010, 0.0079, 0.0069,\n",
    "    0.0011, 0.0005, 0.0004\n",
    "]\n",
    "\n",
    "val_losses = [\n",
    "    0.0292, 0.0292, 0.0292, \n",
    "    0.0605, 0.0605, 0.0605,\n",
    "    0.1548, 0.1548, 0.1548,\n",
    "    0.1823, 0.1823, 0.1823,\n",
    "    0.0139, 0.0139, 0.0139,\n",
    "    0.0166, 0.0166, 0.0166\n",
    "]\n",
    "\n",
    "# Crear el gráfico\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps_32, losses, 'o-', label='Training Loss', color='tab:blue')\n",
    "plt.plot(steps_32, val_losses, 'o-', label='Validation Loss', color='tab:green')\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss per Step')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEfPdV-xkBYY",
    "outputId": "b3936e5f-1a3f-4184-b3fc-e3e7079bc71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sads\n",
      "l1.embeddings.word_embeddings.weight: torch.Size([30522, 1024])\n",
      "l1.embeddings.position_embeddings.weight: torch.Size([512, 1024])\n",
      "l1.embeddings.token_type_embeddings.weight: torch.Size([2, 1024])\n",
      "l1.embeddings.LayerNorm.weight: torch.Size([1024])\n",
      "l1.embeddings.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.0.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.0.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.0.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.0.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.0.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.0.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.0.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.0.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.0.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.1.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.1.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.1.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.1.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.1.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.1.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.1.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.1.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.1.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.2.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.2.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.2.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.2.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.2.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.2.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.2.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.2.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.2.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.3.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.3.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.3.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.3.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.3.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.3.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.3.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.3.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.3.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.4.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.4.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.4.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.4.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.4.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.4.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.4.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.4.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.4.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.5.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.5.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.5.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.5.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.5.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.5.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.5.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.5.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.5.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.6.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.6.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.6.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.6.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.6.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.6.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.6.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.6.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.6.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.7.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.7.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.7.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.7.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.7.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.7.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.7.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.7.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.7.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.8.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.8.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.8.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.8.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.8.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.8.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.8.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.8.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.8.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.9.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.9.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.9.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.9.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.9.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.9.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.9.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.9.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.9.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.10.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.10.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.10.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.10.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.10.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.10.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.10.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.10.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.10.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.11.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.11.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.11.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.11.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.11.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.11.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.11.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.11.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.11.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.12.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.12.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.12.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.12.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.12.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.12.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.12.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.12.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.12.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.12.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.13.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.13.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.13.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.13.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.13.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.13.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.13.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.13.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.13.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.13.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.14.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.14.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.14.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.14.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.14.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.14.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.14.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.14.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.14.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.14.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.15.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.15.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.15.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.15.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.15.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.15.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.15.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.15.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.15.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.15.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.16.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.16.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.16.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.16.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.16.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.16.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.16.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.16.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.16.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.16.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.17.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.17.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.17.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.17.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.17.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.17.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.17.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.17.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.17.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.17.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.18.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.18.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.18.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.18.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.18.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.18.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.18.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.18.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.18.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.18.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.19.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.19.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.19.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.19.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.19.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.19.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.19.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.19.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.19.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.19.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.20.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.20.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.20.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.20.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.20.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.20.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.20.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.20.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.20.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.20.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.21.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.21.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.21.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.21.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.21.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.21.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.21.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.21.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.21.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.21.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.22.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.22.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.22.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.22.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.22.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.22.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.22.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.22.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.22.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.22.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.attention.self.query.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.23.attention.self.query.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.attention.self.key.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.23.attention.self.key.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.attention.self.value.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.23.attention.self.value.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.attention.output.dense.weight: torch.Size([1024, 1024])\n",
      "l1.encoder.layer.23.attention.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.attention.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.23.attention.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.intermediate.dense.weight: torch.Size([4096, 1024])\n",
      "l1.encoder.layer.23.intermediate.dense.bias: torch.Size([4096])\n",
      "l1.encoder.layer.23.output.dense.weight: torch.Size([1024, 4096])\n",
      "l1.encoder.layer.23.output.dense.bias: torch.Size([1024])\n",
      "l1.encoder.layer.23.output.LayerNorm.weight: torch.Size([1024])\n",
      "l1.encoder.layer.23.output.LayerNorm.bias: torch.Size([1024])\n",
      "l1.pooler.dense.weight: torch.Size([1024, 1024])\n",
      "l1.pooler.dense.bias: torch.Size([1024])\n",
      "l3.weight: torch.Size([256, 768])\n",
      "l3.bias: torch.Size([256])\n",
      "l4.weight: torch.Size([2, 256])\n",
      "l4.bias: torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransformerClass:\n\tMissing key(s) in state_dict: \"l1.encoder.layer.12.attention.self.query.weight\", \"l1.encoder.layer.12.attention.self.query.bias\", \"l1.encoder.layer.12.attention.self.key.weight\", \"l1.encoder.layer.12.attention.self.key.bias\", \"l1.encoder.layer.12.attention.self.value.weight\", \"l1.encoder.layer.12.attention.self.value.bias\", \"l1.encoder.layer.12.attention.output.dense.weight\", \"l1.encoder.layer.12.attention.output.dense.bias\", \"l1.encoder.layer.12.attention.output.LayerNorm.weight\", \"l1.encoder.layer.12.attention.output.LayerNorm.bias\", \"l1.encoder.layer.12.intermediate.dense.weight\", \"l1.encoder.layer.12.intermediate.dense.bias\", \"l1.encoder.layer.12.output.dense.weight\", \"l1.encoder.layer.12.output.dense.bias\", \"l1.encoder.layer.12.output.LayerNorm.weight\", \"l1.encoder.layer.12.output.LayerNorm.bias\", \"l1.encoder.layer.13.attention.self.query.weight\", \"l1.encoder.layer.13.attention.self.query.bias\", \"l1.encoder.layer.13.attention.self.key.weight\", \"l1.encoder.layer.13.attention.self.key.bias\", \"l1.encoder.layer.13.attention.self.value.weight\", \"l1.encoder.layer.13.attention.self.value.bias\", \"l1.encoder.layer.13.attention.output.dense.weight\", \"l1.encoder.layer.13.attention.output.dense.bias\", \"l1.encoder.layer.13.attention.output.LayerNorm.weight\", \"l1.encoder.layer.13.attention.output.LayerNorm.bias\", \"l1.encoder.layer.13.intermediate.dense.weight\", \"l1.encoder.layer.13.intermediate.dense.bias\", \"l1.encoder.layer.13.output.dense.weight\", \"l1.encoder.layer.13.output.dense.bias\", \"l1.encoder.layer.13.output.LayerNorm.weight\", \"l1.encoder.layer.13.output.LayerNorm.bias\", \"l1.encoder.layer.14.attention.self.query.weight\", \"l1.encoder.layer.14.attention.self.query.bias\", \"l1.encoder.layer.14.attention.self.key.weight\", \"l1.encoder.layer.14.attention.self.key.bias\", \"l1.encoder.layer.14.attention.self.value.weight\", \"l1.encoder.layer.14.attention.self.value.bias\", \"l1.encoder.layer.14.attention.output.dense.weight\", \"l1.encoder.layer.14.attention.output.dense.bias\", \"l1.encoder.layer.14.attention.output.LayerNorm.weight\", \"l1.encoder.layer.14.attention.output.LayerNorm.bias\", \"l1.encoder.layer.14.intermediate.dense.weight\", \"l1.encoder.layer.14.intermediate.dense.bias\", \"l1.encoder.layer.14.output.dense.weight\", \"l1.encoder.layer.14.output.dense.bias\", \"l1.encoder.layer.14.output.LayerNorm.weight\", \"l1.encoder.layer.14.output.LayerNorm.bias\", \"l1.encoder.layer.15.attention.self.query.weight\", \"l1.encoder.layer.15.attention.self.query.bias\", \"l1.encoder.layer.15.attention.self.key.weight\", \"l1.encoder.layer.15.attention.self.key.bias\", \"l1.encoder.layer.15.attention.self.value.weight\", \"l1.encoder.layer.15.attention.self.value.bias\", \"l1.encoder.layer.15.attention.output.dense.weight\", \"l1.encoder.layer.15.attention.output.dense.bias\", \"l1.encoder.layer.15.attention.output.LayerNorm.weight\", \"l1.encoder.layer.15.attention.output.LayerNorm.bias\", \"l1.encoder.layer.15.intermediate.dense.weight\", \"l1.encoder.layer.15.intermediate.dense.bias\", \"l1.encoder.layer.15.output.dense.weight\", \"l1.encoder.layer.15.output.dense.bias\", \"l1.encoder.layer.15.output.LayerNorm.weight\", \"l1.encoder.layer.15.output.LayerNorm.bias\", \"l1.encoder.layer.16.attention.self.query.weight\", \"l1.encoder.layer.16.attention.self.query.bias\", \"l1.encoder.layer.16.attention.self.key.weight\", \"l1.encoder.layer.16.attention.self.key.bias\", \"l1.encoder.layer.16.attention.self.value.weight\", \"l1.encoder.layer.16.attention.self.value.bias\", \"l1.encoder.layer.16.attention.output.dense.weight\", \"l1.encoder.layer.16.attention.output.dense.bias\", \"l1.encoder.layer.16.attention.output.LayerNorm.weight\", \"l1.encoder.layer.16.attention.output.LayerNorm.bias\", \"l1.encoder.layer.16.intermediate.dense.weight\", \"l1.encoder.layer.16.intermediate.dense.bias\", \"l1.encoder.layer.16.output.dense.weight\", \"l1.encoder.layer.16.output.dense.bias\", \"l1.encoder.layer.16.output.LayerNorm.weight\", \"l1.encoder.layer.16.output.LayerNorm.bias\", \"l1.encoder.layer.17.attention.self.query.weight\", \"l1.encoder.layer.17.attention.self.query.bias\", \"l1.encoder.layer.17.attention.self.key.weight\", \"l1.encoder.layer.17.attention.self.key.bias\", \"l1.encoder.layer.17.attention.self.value.weight\", \"l1.encoder.layer.17.attention.self.value.bias\", \"l1.encoder.layer.17.attention.output.dense.weight\", \"l1.encoder.layer.17.attention.output.dense.bias\", \"l1.encoder.layer.17.attention.output.LayerNorm.weight\", \"l1.encoder.layer.17.attention.output.LayerNorm.bias\", \"l1.encoder.layer.17.intermediate.dense.weight\", \"l1.encoder.layer.17.intermediate.dense.bias\", \"l1.encoder.layer.17.output.dense.weight\", \"l1.encoder.layer.17.output.dense.bias\", \"l1.encoder.layer.17.output.LayerNorm.weight\", \"l1.encoder.layer.17.output.LayerNorm.bias\", \"l1.encoder.layer.18.attention.self.query.weight\", \"l1.encoder.layer.18.attention.self.query.bias\", \"l1.encoder.layer.18.attention.self.key.weight\", \"l1.encoder.layer.18.attention.self.key.bias\", \"l1.encoder.layer.18.attention.self.value.weight\", \"l1.encoder.layer.18.attention.self.value.bias\", \"l1.encoder.layer.18.attention.output.dense.weight\", \"l1.encoder.layer.18.attention.output.dense.bias\", \"l1.encoder.layer.18.attention.output.LayerNorm.weight\", \"l1.encoder.layer.18.attention.output.LayerNorm.bias\", \"l1.encoder.layer.18.intermediate.dense.weight\", \"l1.encoder.layer.18.intermediate.dense.bias\", \"l1.encoder.layer.18.output.dense.weight\", \"l1.encoder.layer.18.output.dense.bias\", \"l1.encoder.layer.18.output.LayerNorm.weight\", \"l1.encoder.layer.18.output.LayerNorm.bias\", \"l1.encoder.layer.19.attention.self.query.weight\", \"l1.encoder.layer.19.attention.self.query.bias\", \"l1.encoder.layer.19.attention.self.key.weight\", \"l1.encoder.layer.19.attention.self.key.bias\", \"l1.encoder.layer.19.attention.self.value.weight\", \"l1.encoder.layer.19.attention.self.value.bias\", \"l1.encoder.layer.19.attention.output.dense.weight\", \"l1.encoder.layer.19.attention.output.dense.bias\", \"l1.encoder.layer.19.attention.output.LayerNorm.weight\", \"l1.encoder.layer.19.attention.output.LayerNorm.bias\", \"l1.encoder.layer.19.intermediate.dense.weight\", \"l1.encoder.layer.19.intermediate.dense.bias\", \"l1.encoder.layer.19.output.dense.weight\", \"l1.encoder.layer.19.output.dense.bias\", \"l1.encoder.layer.19.output.LayerNorm.weight\", \"l1.encoder.layer.19.output.LayerNorm.bias\", \"l1.encoder.layer.20.attention.self.query.weight\", \"l1.encoder.layer.20.attention.self.query.bias\", \"l1.encoder.layer.20.attention.self.key.weight\", \"l1.encoder.layer.20.attention.self.key.bias\", \"l1.encoder.layer.20.attention.self.value.weight\", \"l1.encoder.layer.20.attention.self.value.bias\", \"l1.encoder.layer.20.attention.output.dense.weight\", \"l1.encoder.layer.20.attention.output.dense.bias\", \"l1.encoder.layer.20.attention.output.LayerNorm.weight\", \"l1.encoder.layer.20.attention.output.LayerNorm.bias\", \"l1.encoder.layer.20.intermediate.dense.weight\", \"l1.encoder.layer.20.intermediate.dense.bias\", \"l1.encoder.layer.20.output.dense.weight\", \"l1.encoder.layer.20.output.dense.bias\", \"l1.encoder.layer.20.output.LayerNorm.weight\", \"l1.encoder.layer.20.output.LayerNorm.bias\", \"l1.encoder.layer.21.attention.self.query.weight\", \"l1.encoder.layer.21.attention.self.query.bias\", \"l1.encoder.layer.21.attention.self.key.weight\", \"l1.encoder.layer.21.attention.self.key.bias\", \"l1.encoder.layer.21.attention.self.value.weight\", \"l1.encoder.layer.21.attention.self.value.bias\", \"l1.encoder.layer.21.attention.output.dense.weight\", \"l1.encoder.layer.21.attention.output.dense.bias\", \"l1.encoder.layer.21.attention.output.LayerNorm.weight\", \"l1.encoder.layer.21.attention.output.LayerNorm.bias\", \"l1.encoder.layer.21.intermediate.dense.weight\", \"l1.encoder.layer.21.intermediate.dense.bias\", \"l1.encoder.layer.21.output.dense.weight\", \"l1.encoder.layer.21.output.dense.bias\", \"l1.encoder.layer.21.output.LayerNorm.weight\", \"l1.encoder.layer.21.output.LayerNorm.bias\", \"l1.encoder.layer.22.attention.self.query.weight\", \"l1.encoder.layer.22.attention.self.query.bias\", \"l1.encoder.layer.22.attention.self.key.weight\", \"l1.encoder.layer.22.attention.self.key.bias\", \"l1.encoder.layer.22.attention.self.value.weight\", \"l1.encoder.layer.22.attention.self.value.bias\", \"l1.encoder.layer.22.attention.output.dense.weight\", \"l1.encoder.layer.22.attention.output.dense.bias\", \"l1.encoder.layer.22.attention.output.LayerNorm.weight\", \"l1.encoder.layer.22.attention.output.LayerNorm.bias\", \"l1.encoder.layer.22.intermediate.dense.weight\", \"l1.encoder.layer.22.intermediate.dense.bias\", \"l1.encoder.layer.22.output.dense.weight\", \"l1.encoder.layer.22.output.dense.bias\", \"l1.encoder.layer.22.output.LayerNorm.weight\", \"l1.encoder.layer.22.output.LayerNorm.bias\", \"l1.encoder.layer.23.attention.self.query.weight\", \"l1.encoder.layer.23.attention.self.query.bias\", \"l1.encoder.layer.23.attention.self.key.weight\", \"l1.encoder.layer.23.attention.self.key.bias\", \"l1.encoder.layer.23.attention.self.value.weight\", \"l1.encoder.layer.23.attention.self.value.bias\", \"l1.encoder.layer.23.attention.output.dense.weight\", \"l1.encoder.layer.23.attention.output.dense.bias\", \"l1.encoder.layer.23.attention.output.LayerNorm.weight\", \"l1.encoder.layer.23.attention.output.LayerNorm.bias\", \"l1.encoder.layer.23.intermediate.dense.weight\", \"l1.encoder.layer.23.intermediate.dense.bias\", \"l1.encoder.layer.23.output.dense.weight\", \"l1.encoder.layer.23.output.dense.bias\", \"l1.encoder.layer.23.output.LayerNorm.weight\", \"l1.encoder.layer.23.output.LayerNorm.bias\". \n\tsize mismatch for l1.embeddings.word_embeddings.weight: copying a param with shape torch.Size([30522, 768]) from checkpoint, the shape in current model is torch.Size([30522, 1024]).\n\tsize mismatch for l1.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for l1.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 1024]).\n\tsize mismatch for l1.embeddings.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.embeddings.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.pooler.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.pooler.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBERT_B_Embedding\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_epoch_5_acc0.9965.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\github\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerClass:\n\tMissing key(s) in state_dict: \"l1.encoder.layer.12.attention.self.query.weight\", \"l1.encoder.layer.12.attention.self.query.bias\", \"l1.encoder.layer.12.attention.self.key.weight\", \"l1.encoder.layer.12.attention.self.key.bias\", \"l1.encoder.layer.12.attention.self.value.weight\", \"l1.encoder.layer.12.attention.self.value.bias\", \"l1.encoder.layer.12.attention.output.dense.weight\", \"l1.encoder.layer.12.attention.output.dense.bias\", \"l1.encoder.layer.12.attention.output.LayerNorm.weight\", \"l1.encoder.layer.12.attention.output.LayerNorm.bias\", \"l1.encoder.layer.12.intermediate.dense.weight\", \"l1.encoder.layer.12.intermediate.dense.bias\", \"l1.encoder.layer.12.output.dense.weight\", \"l1.encoder.layer.12.output.dense.bias\", \"l1.encoder.layer.12.output.LayerNorm.weight\", \"l1.encoder.layer.12.output.LayerNorm.bias\", \"l1.encoder.layer.13.attention.self.query.weight\", \"l1.encoder.layer.13.attention.self.query.bias\", \"l1.encoder.layer.13.attention.self.key.weight\", \"l1.encoder.layer.13.attention.self.key.bias\", \"l1.encoder.layer.13.attention.self.value.weight\", \"l1.encoder.layer.13.attention.self.value.bias\", \"l1.encoder.layer.13.attention.output.dense.weight\", \"l1.encoder.layer.13.attention.output.dense.bias\", \"l1.encoder.layer.13.attention.output.LayerNorm.weight\", \"l1.encoder.layer.13.attention.output.LayerNorm.bias\", \"l1.encoder.layer.13.intermediate.dense.weight\", \"l1.encoder.layer.13.intermediate.dense.bias\", \"l1.encoder.layer.13.output.dense.weight\", \"l1.encoder.layer.13.output.dense.bias\", \"l1.encoder.layer.13.output.LayerNorm.weight\", \"l1.encoder.layer.13.output.LayerNorm.bias\", \"l1.encoder.layer.14.attention.self.query.weight\", \"l1.encoder.layer.14.attention.self.query.bias\", \"l1.encoder.layer.14.attention.self.key.weight\", \"l1.encoder.layer.14.attention.self.key.bias\", \"l1.encoder.layer.14.attention.self.value.weight\", \"l1.encoder.layer.14.attention.self.value.bias\", \"l1.encoder.layer.14.attention.output.dense.weight\", \"l1.encoder.layer.14.attention.output.dense.bias\", \"l1.encoder.layer.14.attention.output.LayerNorm.weight\", \"l1.encoder.layer.14.attention.output.LayerNorm.bias\", \"l1.encoder.layer.14.intermediate.dense.weight\", \"l1.encoder.layer.14.intermediate.dense.bias\", \"l1.encoder.layer.14.output.dense.weight\", \"l1.encoder.layer.14.output.dense.bias\", \"l1.encoder.layer.14.output.LayerNorm.weight\", \"l1.encoder.layer.14.output.LayerNorm.bias\", \"l1.encoder.layer.15.attention.self.query.weight\", \"l1.encoder.layer.15.attention.self.query.bias\", \"l1.encoder.layer.15.attention.self.key.weight\", \"l1.encoder.layer.15.attention.self.key.bias\", \"l1.encoder.layer.15.attention.self.value.weight\", \"l1.encoder.layer.15.attention.self.value.bias\", \"l1.encoder.layer.15.attention.output.dense.weight\", \"l1.encoder.layer.15.attention.output.dense.bias\", \"l1.encoder.layer.15.attention.output.LayerNorm.weight\", \"l1.encoder.layer.15.attention.output.LayerNorm.bias\", \"l1.encoder.layer.15.intermediate.dense.weight\", \"l1.encoder.layer.15.intermediate.dense.bias\", \"l1.encoder.layer.15.output.dense.weight\", \"l1.encoder.layer.15.output.dense.bias\", \"l1.encoder.layer.15.output.LayerNorm.weight\", \"l1.encoder.layer.15.output.LayerNorm.bias\", \"l1.encoder.layer.16.attention.self.query.weight\", \"l1.encoder.layer.16.attention.self.query.bias\", \"l1.encoder.layer.16.attention.self.key.weight\", \"l1.encoder.layer.16.attention.self.key.bias\", \"l1.encoder.layer.16.attention.self.value.weight\", \"l1.encoder.layer.16.attention.self.value.bias\", \"l1.encoder.layer.16.attention.output.dense.weight\", \"l1.encoder.layer.16.attention.output.dense.bias\", \"l1.encoder.layer.16.attention.output.LayerNorm.weight\", \"l1.encoder.layer.16.attention.output.LayerNorm.bias\", \"l1.encoder.layer.16.intermediate.dense.weight\", \"l1.encoder.layer.16.intermediate.dense.bias\", \"l1.encoder.layer.16.output.dense.weight\", \"l1.encoder.layer.16.output.dense.bias\", \"l1.encoder.layer.16.output.LayerNorm.weight\", \"l1.encoder.layer.16.output.LayerNorm.bias\", \"l1.encoder.layer.17.attention.self.query.weight\", \"l1.encoder.layer.17.attention.self.query.bias\", \"l1.encoder.layer.17.attention.self.key.weight\", \"l1.encoder.layer.17.attention.self.key.bias\", \"l1.encoder.layer.17.attention.self.value.weight\", \"l1.encoder.layer.17.attention.self.value.bias\", \"l1.encoder.layer.17.attention.output.dense.weight\", \"l1.encoder.layer.17.attention.output.dense.bias\", \"l1.encoder.layer.17.attention.output.LayerNorm.weight\", \"l1.encoder.layer.17.attention.output.LayerNorm.bias\", \"l1.encoder.layer.17.intermediate.dense.weight\", \"l1.encoder.layer.17.intermediate.dense.bias\", \"l1.encoder.layer.17.output.dense.weight\", \"l1.encoder.layer.17.output.dense.bias\", \"l1.encoder.layer.17.output.LayerNorm.weight\", \"l1.encoder.layer.17.output.LayerNorm.bias\", \"l1.encoder.layer.18.attention.self.query.weight\", \"l1.encoder.layer.18.attention.self.query.bias\", \"l1.encoder.layer.18.attention.self.key.weight\", \"l1.encoder.layer.18.attention.self.key.bias\", \"l1.encoder.layer.18.attention.self.value.weight\", \"l1.encoder.layer.18.attention.self.value.bias\", \"l1.encoder.layer.18.attention.output.dense.weight\", \"l1.encoder.layer.18.attention.output.dense.bias\", \"l1.encoder.layer.18.attention.output.LayerNorm.weight\", \"l1.encoder.layer.18.attention.output.LayerNorm.bias\", \"l1.encoder.layer.18.intermediate.dense.weight\", \"l1.encoder.layer.18.intermediate.dense.bias\", \"l1.encoder.layer.18.output.dense.weight\", \"l1.encoder.layer.18.output.dense.bias\", \"l1.encoder.layer.18.output.LayerNorm.weight\", \"l1.encoder.layer.18.output.LayerNorm.bias\", \"l1.encoder.layer.19.attention.self.query.weight\", \"l1.encoder.layer.19.attention.self.query.bias\", \"l1.encoder.layer.19.attention.self.key.weight\", \"l1.encoder.layer.19.attention.self.key.bias\", \"l1.encoder.layer.19.attention.self.value.weight\", \"l1.encoder.layer.19.attention.self.value.bias\", \"l1.encoder.layer.19.attention.output.dense.weight\", \"l1.encoder.layer.19.attention.output.dense.bias\", \"l1.encoder.layer.19.attention.output.LayerNorm.weight\", \"l1.encoder.layer.19.attention.output.LayerNorm.bias\", \"l1.encoder.layer.19.intermediate.dense.weight\", \"l1.encoder.layer.19.intermediate.dense.bias\", \"l1.encoder.layer.19.output.dense.weight\", \"l1.encoder.layer.19.output.dense.bias\", \"l1.encoder.layer.19.output.LayerNorm.weight\", \"l1.encoder.layer.19.output.LayerNorm.bias\", \"l1.encoder.layer.20.attention.self.query.weight\", \"l1.encoder.layer.20.attention.self.query.bias\", \"l1.encoder.layer.20.attention.self.key.weight\", \"l1.encoder.layer.20.attention.self.key.bias\", \"l1.encoder.layer.20.attention.self.value.weight\", \"l1.encoder.layer.20.attention.self.value.bias\", \"l1.encoder.layer.20.attention.output.dense.weight\", \"l1.encoder.layer.20.attention.output.dense.bias\", \"l1.encoder.layer.20.attention.output.LayerNorm.weight\", \"l1.encoder.layer.20.attention.output.LayerNorm.bias\", \"l1.encoder.layer.20.intermediate.dense.weight\", \"l1.encoder.layer.20.intermediate.dense.bias\", \"l1.encoder.layer.20.output.dense.weight\", \"l1.encoder.layer.20.output.dense.bias\", \"l1.encoder.layer.20.output.LayerNorm.weight\", \"l1.encoder.layer.20.output.LayerNorm.bias\", \"l1.encoder.layer.21.attention.self.query.weight\", \"l1.encoder.layer.21.attention.self.query.bias\", \"l1.encoder.layer.21.attention.self.key.weight\", \"l1.encoder.layer.21.attention.self.key.bias\", \"l1.encoder.layer.21.attention.self.value.weight\", \"l1.encoder.layer.21.attention.self.value.bias\", \"l1.encoder.layer.21.attention.output.dense.weight\", \"l1.encoder.layer.21.attention.output.dense.bias\", \"l1.encoder.layer.21.attention.output.LayerNorm.weight\", \"l1.encoder.layer.21.attention.output.LayerNorm.bias\", \"l1.encoder.layer.21.intermediate.dense.weight\", \"l1.encoder.layer.21.intermediate.dense.bias\", \"l1.encoder.layer.21.output.dense.weight\", \"l1.encoder.layer.21.output.dense.bias\", \"l1.encoder.layer.21.output.LayerNorm.weight\", \"l1.encoder.layer.21.output.LayerNorm.bias\", \"l1.encoder.layer.22.attention.self.query.weight\", \"l1.encoder.layer.22.attention.self.query.bias\", \"l1.encoder.layer.22.attention.self.key.weight\", \"l1.encoder.layer.22.attention.self.key.bias\", \"l1.encoder.layer.22.attention.self.value.weight\", \"l1.encoder.layer.22.attention.self.value.bias\", \"l1.encoder.layer.22.attention.output.dense.weight\", \"l1.encoder.layer.22.attention.output.dense.bias\", \"l1.encoder.layer.22.attention.output.LayerNorm.weight\", \"l1.encoder.layer.22.attention.output.LayerNorm.bias\", \"l1.encoder.layer.22.intermediate.dense.weight\", \"l1.encoder.layer.22.intermediate.dense.bias\", \"l1.encoder.layer.22.output.dense.weight\", \"l1.encoder.layer.22.output.dense.bias\", \"l1.encoder.layer.22.output.LayerNorm.weight\", \"l1.encoder.layer.22.output.LayerNorm.bias\", \"l1.encoder.layer.23.attention.self.query.weight\", \"l1.encoder.layer.23.attention.self.query.bias\", \"l1.encoder.layer.23.attention.self.key.weight\", \"l1.encoder.layer.23.attention.self.key.bias\", \"l1.encoder.layer.23.attention.self.value.weight\", \"l1.encoder.layer.23.attention.self.value.bias\", \"l1.encoder.layer.23.attention.output.dense.weight\", \"l1.encoder.layer.23.attention.output.dense.bias\", \"l1.encoder.layer.23.attention.output.LayerNorm.weight\", \"l1.encoder.layer.23.attention.output.LayerNorm.bias\", \"l1.encoder.layer.23.intermediate.dense.weight\", \"l1.encoder.layer.23.intermediate.dense.bias\", \"l1.encoder.layer.23.output.dense.weight\", \"l1.encoder.layer.23.output.dense.bias\", \"l1.encoder.layer.23.output.LayerNorm.weight\", \"l1.encoder.layer.23.output.LayerNorm.bias\". \n\tsize mismatch for l1.embeddings.word_embeddings.weight: copying a param with shape torch.Size([30522, 768]) from checkpoint, the shape in current model is torch.Size([30522, 1024]).\n\tsize mismatch for l1.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for l1.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 1024]).\n\tsize mismatch for l1.embeddings.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.embeddings.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for l1.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for l1.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for l1.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for l1.pooler.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for l1.pooler.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024])."
     ]
    }
   ],
   "source": [
    "model = TransformerClass()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "\n",
    "model_save_path = f'..\\model_weights\\BERT_B_Embedding\\model_epoch_5_acc0.9965.pth'\n",
    "model.load_state_dict(torch.load(model_save_path, weights_only=False))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eavl0eh5j1f0",
    "outputId": "1146832a-8f39-4ce5-8b87-4306d7ea1560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0145\n",
      "Test Accuracy: 0.9951\n",
      "ROC-AUC: 0.9999\n",
      "Brier Score: 0.0041\n",
      "C@1: 0.9951\n",
      "F1 Score: 0.9951\n",
      "F0.5 Score: 0.9951\n"
     ]
    }
   ],
   "source": [
    "def test(test_loader, model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            input_ids = data['ids'].to(device)\n",
    "            attention_mask = data['mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            test_loss += torch.nn.functional.cross_entropy(logits, targets, reduction='sum').item()\n",
    "\n",
    "            # Predicciones y probabilidades\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Acumulamos\n",
    "            all_probs.extend(probs.cpu().numpy()[:, 1])  # Probabilidades de clase positiva\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.argmax(dim=1).cpu().numpy())  # Para multi-clase, usa el índice\n",
    "\n",
    "            correct_predictions += (preds == targets.argmax(dim=1)).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / total_predictions\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Métricas adicionales\n",
    "    roc_auc = roc_auc_score(all_targets, all_probs)\n",
    "    brier = brier_score_loss(all_targets, all_probs)\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')  # Ajusta el promedio si es necesario\n",
    "    f05 = precision_recall_fscore_support(all_targets, all_preds, average='weighted', beta=0.5)[2]\n",
    "\n",
    "    # C@1\n",
    "    correct = sum(1 for t, p in zip(all_targets, all_preds) if t == p)\n",
    "    c_at_1 = correct / len(all_preds)\n",
    "\n",
    "    return avg_test_loss, accuracy, roc_auc, brier, c_at_1, f1, f05\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "test_loss, test_accuracy, roc_auc, brier, c_at_1, f1, f05 = test(test_loader, model, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Brier Score: {brier:.4f}\")\n",
    "print(f\"C@1: {c_at_1:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"F0.5 Score: {f05:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
