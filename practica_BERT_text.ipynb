{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2jLpZhWxc4wV"
   },
   "outputs": [],
   "source": [
    "#!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ijaKWX_ecbac",
    "outputId": "8d5bcab8-8fae-4119-bc00-535d4ef89622"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\github\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuración de dispositivo (GPU o CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q_zVgjhtcbae"
   },
   "outputs": [],
   "source": [
    "# @title Customize your key variables here\n",
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200 # @param {type:\"integer\"}\n",
    "TRAIN_BATCH_SIZE = 16 # @param {type:\"integer\"}\n",
    "VALID_BATCH_SIZE = 16 # @param {type:\"integer\"}\n",
    "EPOCHS = 4 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 1e-4 # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGICgd8wcbae",
    "outputId": "21ca6e93-8db1-4ba7-cf32-983174f34156"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines_files = glob('./data/machine/*.jsonl')\n",
    "len(machines_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "Ouq8Ii2lcbae",
    "outputId": "b671c7f6-9177-437f-846f-9e51effa9b8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>target_human</th>\n",
       "      <th>target_machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>Inaugural Address: President Joseph R. Biden J...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>What should be the focus of the speech? The In...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>Biden's Inaugural Address Highlights Triumph o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>Biden's Inaugural Address: A Clarion Call for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news-2021-01-01-2021-12-31-bideninauguration/a...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>President Biden Emphasizes Unity, Democracy, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>Gabby Petito: Long Island Surf Shop Owner Reme...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>Gabby Petito: Surf Shop Owner in Hometown Reme...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>Gabby Petito Remembered as a 'Kind-Hearted Sou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14129</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>Gabby Petito Remembered as a 'Super Kind-Heart...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>news-2021-01-01-2021-12-31-wyominggabbypetito/...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>A Very Kind and Sweet Woman in Long Island Sho...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id  \\\n",
       "0      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "1      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "2      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "3      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "4      news-2021-01-01-2021-12-31-bideninauguration/a...   \n",
       "...                                                  ...   \n",
       "14126  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14127  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14128  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14129  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "14130  news-2021-01-01-2021-12-31-wyominggabbypetito/...   \n",
       "\n",
       "                                                  text_1  \\\n",
       "0      Inaugural Address by President Joseph R. Biden...   \n",
       "1      Inaugural Address by President Joseph R. Biden...   \n",
       "2      Inaugural Address by President Joseph R. Biden...   \n",
       "3      Inaugural Address by President Joseph R. Biden...   \n",
       "4      Inaugural Address by President Joseph R. Biden...   \n",
       "...                                                  ...   \n",
       "14126  Gabby Petito case: Surf shop owner in her home...   \n",
       "14127  Gabby Petito case: Surf shop owner in her home...   \n",
       "14128  Gabby Petito case: Surf shop owner in her home...   \n",
       "14129  Gabby Petito case: Surf shop owner in her home...   \n",
       "14130  Gabby Petito case: Surf shop owner in her home...   \n",
       "\n",
       "                                                  text_2  target_human  \\\n",
       "0      Inaugural Address: President Joseph R. Biden J...             1   \n",
       "1      What should be the focus of the speech? The In...             1   \n",
       "2      Biden's Inaugural Address Highlights Triumph o...             1   \n",
       "3      Biden's Inaugural Address: A Clarion Call for ...             1   \n",
       "4      President Biden Emphasizes Unity, Democracy, a...             1   \n",
       "...                                                  ...           ...   \n",
       "14126  Gabby Petito: Long Island Surf Shop Owner Reme...             1   \n",
       "14127  Gabby Petito: Surf Shop Owner in Hometown Reme...             1   \n",
       "14128  Gabby Petito Remembered as a 'Kind-Hearted Sou...             1   \n",
       "14129  Gabby Petito Remembered as a 'Super Kind-Heart...             1   \n",
       "14130  A Very Kind and Sweet Woman in Long Island Sho...             1   \n",
       "\n",
       "       target_machine  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "14126               0  \n",
       "14127               0  \n",
       "14128               0  \n",
       "14129               0  \n",
       "14130               0  \n",
       "\n",
       "[14131 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human = pd.read_json('./data/human.jsonl', lines=True)\n",
    "\n",
    "df_machine = pd.read_json(machines_files[0], lines=True)\n",
    "for file in machines_files[1:]:\n",
    "    df_current = pd.read_json(file, lines=True)\n",
    "    df_machine = pd.concat([df_machine, df_current])\n",
    "\n",
    "\n",
    "df_human[\"id\"] = df_human[\"id\"].str.split('/').str[1:].str.join('/')\n",
    "df_machine[\"id\"] = df_machine[\"id\"].str.split('/').str[1:].str.join('/')\n",
    "\n",
    "df_combined = pd.merge(df_human, df_machine, on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "df_combined['target_human'] = 1\n",
    "df_combined['target_machine'] = 0\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "CB8z35Ggcbae",
    "outputId": "d0dccaae-b1db-4720-c8ba-f2f24ab40231"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>target_tuple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaugural Address: President Joseph R. Biden J...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>What should be the focus of the speech? The In...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>Biden's Inaugural Address Highlights Triumph o...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden's Inaugural Address: A Clarion Call for ...</td>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inaugural Address by President Joseph R. Biden...</td>\n",
       "      <td>President Biden Emphasizes Unity, Democracy, a...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>Gabby Petito: Long Island Surf Shop Owner Reme...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>Gabby Petito: Surf Shop Owner in Hometown Reme...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>Gabby Petito Remembered as a 'Kind-Hearted Sou...</td>\n",
       "      <td>(1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14129</th>\n",
       "      <td>Gabby Petito Remembered as a 'Super Kind-Heart...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>A Very Kind and Sweet Woman in Long Island Sho...</td>\n",
       "      <td>Gabby Petito case: Surf shop owner in her home...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14131 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text_1  \\\n",
       "0      Inaugural Address: President Joseph R. Biden J...   \n",
       "1      Inaugural Address by President Joseph R. Biden...   \n",
       "2      Inaugural Address by President Joseph R. Biden...   \n",
       "3      Biden's Inaugural Address: A Clarion Call for ...   \n",
       "4      Inaugural Address by President Joseph R. Biden...   \n",
       "...                                                  ...   \n",
       "14126  Gabby Petito: Long Island Surf Shop Owner Reme...   \n",
       "14127  Gabby Petito case: Surf shop owner in her home...   \n",
       "14128  Gabby Petito case: Surf shop owner in her home...   \n",
       "14129  Gabby Petito Remembered as a 'Super Kind-Heart...   \n",
       "14130  A Very Kind and Sweet Woman in Long Island Sho...   \n",
       "\n",
       "                                                  text_2 target_tuple  \n",
       "0      Inaugural Address by President Joseph R. Biden...       (0, 1)  \n",
       "1      What should be the focus of the speech? The In...       (1, 0)  \n",
       "2      Biden's Inaugural Address Highlights Triumph o...       (1, 0)  \n",
       "3      Inaugural Address by President Joseph R. Biden...       (0, 1)  \n",
       "4      President Biden Emphasizes Unity, Democracy, a...       (1, 0)  \n",
       "...                                                  ...          ...  \n",
       "14126  Gabby Petito case: Surf shop owner in her home...       (0, 1)  \n",
       "14127  Gabby Petito: Surf Shop Owner in Hometown Reme...       (1, 0)  \n",
       "14128  Gabby Petito Remembered as a 'Kind-Hearted Sou...       (1, 0)  \n",
       "14129  Gabby Petito case: Surf shop owner in her home...       (0, 1)  \n",
       "14130  Gabby Petito case: Surf shop owner in her home...       (0, 1)  \n",
       "\n",
       "[14131 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_indices = df_combined.sample(frac=0.5, random_state=42).index\n",
    "df_combined.loc[random_indices, ['text_1', 'text_2']] = df_combined.loc[random_indices, ['text_2', 'text_1']].values\n",
    "df_combined.loc[random_indices, ['target_human']] = 0\n",
    "df_combined.loc[random_indices, ['target_machine']] = 1\n",
    "df_combined['target_tuple'] = list(zip(df_combined['target_human'], df_combined['target_machine']))\n",
    "df_combined.drop(columns=['id', 'target_human', 'target_machine'], inplace=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rsuB0RLcbaf",
    "outputId": "0bd62678-67bf-4312-afc8-c55b042f648f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9891, 2), (2826, 2), (1414, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_combined[[\"text_1\", \"text_2\"]], df_combined[\"target_tuple\"], test_size=0.3, random_state=43, stratify=df_combined[\"target_tuple\"])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=1/3, random_state=43, stratify=y_val)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LjuGLfavcbaf"
   },
   "outputs": [],
   "source": [
    "class AiClassificationDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Use iloc to access the rows by index for data and labels\n",
    "        text_1 = self.data.iloc[index]['text_1']\n",
    "        text_2 = self.data.iloc[index]['text_2']\n",
    "        target = self.labels.iloc[index]  # assuming labels are in a compatible format\n",
    "        return {\n",
    "            'text_1': text_1,\n",
    "            'text_2': text_2,\n",
    "            'targets': target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3U3xQ8HPcbaf"
   },
   "outputs": [],
   "source": [
    "class AiClassificationCollator:\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataset\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, input_batch):\n",
    "        batch_dict = {colname: [x[colname] for x in input_batch] for colname in input_batch[0]}\n",
    "\n",
    "        # Process text_1\n",
    "        comment_text_1 = batch_dict['text_1']\n",
    "        # print(comment_text_1)\n",
    "        comment_text_1 = [\" \".join(text.split()) for text in comment_text_1]\n",
    "        # print(comment_text_1)\n",
    "\n",
    "        # Process text_2\n",
    "        comment_text_2 = batch_dict['text_2']\n",
    "        comment_text_2 = [\" \".join(text.split()) for text in comment_text_2]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            comment_text_1,\n",
    "            comment_text_2,\n",
    "            max_length=self.max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n",
    "            'targets': torch.tensor(batch_dict['targets'], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8V7aggMfcbaf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qXmpQsNzcbaf"
   },
   "outputs": [],
   "source": [
    "training_set = AiClassificationDataset(X_train, y_train)\n",
    "validation_set = AiClassificationDataset(X_val, y_val)\n",
    "test_set = AiClassificationDataset(X_test, y_test)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': AiClassificationCollator(training_set, tokenizer, MAX_LEN)\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': AiClassificationCollator(validation_set, tokenizer, MAX_LEN)\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': AiClassificationCollator(test_set, tokenizer, MAX_LEN)\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KCmZiciwcbaf"
   },
   "outputs": [],
   "source": [
    "class TransformerClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 256)\n",
    "        self.l4 = torch.nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        last_hidden_state = self.l1(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ).last_hidden_state\n",
    "\n",
    "        cls_token = last_hidden_state[:, 0]\n",
    "        hidden_output = F.gelu(self.l3(self.l2(cls_token)))\n",
    "        output = self.l4(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerClass(torch.nn.Module):\n",
    "#  def __init__(self):\n",
    "#     super(TransformerClass, self).__init__()\n",
    "#     self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "#     self.l2 = torch.nn.Linear(768, 768)\n",
    "#     self.l3 = torch.nn.Dropout(0.1)\n",
    "#     self.l4 = torch.nn.CosineSimilarity(dim=1)\n",
    "#     self.l5 = torch.nn.Linear(1, 1)\n",
    "    \n",
    "#  def forward(self, ids_0, mask_0, token_type_ids_0, ids_1, mask_1, token_type_ids_1):\n",
    "#     last_hidden_state_a = self.l1(ids_0, attention_mask=mask_0, token_type_ids=token_type_ids_0).last_hidden_state[:, 0]\n",
    "#     last_hidden_state_b = self.l1(ids_1, attention_mask=mask_1, token_type_ids=token_type_ids_1).last_hidden_state[:, 0]\n",
    "#     x_a, x_b = self.l2(last_hidden_state_a), self.l2(last_hidden_state_b)\n",
    "#     x_a, x_b = torch.gelu(self.l3(x_a)), torch.gelu(self.l3(x_b))\n",
    "#     sem_sim = self.l4(x_a, x_b)\n",
    "#     weighted_sem_sim = self.l5(sem_sim)\n",
    "#     return weighted_sem_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EsVknC-0eJPY"
   },
   "outputs": [],
   "source": [
    "def training_step(input_ids, attention_mask, token_type_ids, y, model, optimizer):\n",
    "    logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y, reduction='mean')\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ypAtTnOtcbag"
   },
   "outputs": [],
   "source": [
    "model = TransformerClass()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QvHAB-s6cbag"
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            input_ids = data['ids'].to(device)\n",
    "            attention_mask = data['mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            val_loss += torch.nn.functional.cross_entropy(logits, targets, reduction='sum').item()  # Accumulate validation loss\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == targets.argmax(dim=1)).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / total_predictions\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ClviN54hFd6",
    "outputId": "fa97052b-3582-43bc-a74e-e37e5d035441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Step 1/619\n",
      "  Running Loss: 0.6878\n",
      "Epoch 1/4, Step 201/619\n",
      "  Running Loss: 0.2178\n",
      "Epoch 1/4, Step 401/619\n",
      "  Running Loss: 0.1649\n",
      "Epoch 1/4, Step 601/619\n",
      "  Running Loss: 0.1311\n",
      "Epoch 1/4 - End of epoch\n",
      "  Training Loss: 0.1275\n",
      "  Validation Loss: 0.0371\n",
      "  Validation Accuracy: 0.9919\n",
      "Model saved to ./model_weights\\model_epoch_1.pth\n",
      "Epoch 2/4, Step 1/619\n",
      "  Running Loss: 0.0393\n",
      "Epoch 2/4, Step 201/619\n",
      "  Running Loss: 0.0286\n",
      "Epoch 2/4, Step 401/619\n",
      "  Running Loss: 0.0255\n",
      "Epoch 2/4, Step 601/619\n",
      "  Running Loss: 0.0247\n",
      "Epoch 2/4 - End of epoch\n",
      "  Training Loss: 0.0244\n",
      "  Validation Loss: 0.0171\n",
      "  Validation Accuracy: 0.9958\n",
      "Model saved to ./model_weights\\model_epoch_2.pth\n",
      "Epoch 3/4, Step 1/619\n",
      "  Running Loss: 0.0005\n",
      "Epoch 3/4, Step 201/619\n",
      "  Running Loss: 0.0311\n",
      "Epoch 3/4, Step 401/619\n",
      "  Running Loss: 0.0474\n",
      "Epoch 3/4, Step 601/619\n",
      "  Running Loss: 0.0427\n",
      "Epoch 3/4 - End of epoch\n",
      "  Training Loss: 0.0421\n",
      "  Validation Loss: 0.0340\n",
      "  Validation Accuracy: 0.9904\n",
      "Model saved to ./model_weights\\model_epoch_3.pth\n",
      "Epoch 4/4, Step 1/619\n",
      "  Running Loss: 0.0032\n",
      "Epoch 4/4, Step 201/619\n",
      "  Running Loss: 0.0329\n",
      "Epoch 4/4, Step 401/619\n",
      "  Running Loss: 0.0317\n",
      "Epoch 4/4, Step 601/619\n",
      "  Running Loss: 0.0403\n",
      "Epoch 4/4 - End of epoch\n",
      "  Training Loss: 0.0407\n",
      "  Validation Loss: 0.0351\n",
      "  Validation Accuracy: 0.9922\n",
      "Model saved to ./model_weights\\model_epoch_4.pth\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, log_interval=200, save_model_path='./model_weights'):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for step, data in enumerate(training_loader):\n",
    "        input_ids = data['ids'].to(device)\n",
    "        attention_mask = data['mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        loss = training_step(input_ids, attention_mask, token_type_ids, targets, model, optimizer)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = running_loss / (step + 1)\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}, Step {step + 1}/{len(training_loader)}\")\n",
    "            print(f\"  Running Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(training_loader)\n",
    "\n",
    "    avg_val_loss, val_accuracy = validate()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - End of epoch\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "\n",
    "    model_save_path = os.path.join(save_model_path, f\"model_epoch_{epoch + 1}.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEfPdV-xkBYY",
    "outputId": "b3936e5f-1a3f-4184-b3fc-e3e7079bc71b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19944\\855677532.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (l4): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClass()\n",
    "\n",
    "model_save_path = './model_weights/model_epoch_4.pth'\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eavl0eh5j1f0",
    "outputId": "1146832a-8f39-4ce5-8b87-4306d7ea1560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0319\n",
      "Test Accuracy: 0.9880\n"
     ]
    }
   ],
   "source": [
    "def test(test_loader, model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            input_ids = data['ids'].to(device)\n",
    "            attention_mask = data['mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "          \n",
    "            test_loss += torch.nn.functional.cross_entropy(logits, targets, reduction='sum').item()\n",
    "\n",
    "            # Predicciones\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == targets.argmax(dim=1)).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / total_predictions\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = test(test_loader, model, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
